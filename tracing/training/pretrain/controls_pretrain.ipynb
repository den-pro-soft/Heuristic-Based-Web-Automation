{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tracing.rl.actions import *\n",
    "from tracing.rl.a3cmodel import A3CModel\n",
    "from tracing.rl.rewards import PopupRewardsCalculator\n",
    "from tracing.rl.environment import Environment\n",
    "from tracing.rl.actor_learner import ActionsMemory\n",
    "from tracing.rl.actor_learner import ActorLearnerWorker\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import threading\n",
    "import json\n",
    "import random\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "\n",
    "from tracing.selenium_utils.controls import Types\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(arr, batch_size):\n",
    "    batch = []\n",
    "    for item in arr:\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "        batch.append(item)\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "        \n",
    "        \n",
    "def read_img(ctrl, size = 300):\n",
    "    img = misc.imread(ctrl['img_file'])\n",
    "    img = (img - 128.)/128.\n",
    "    shape = img.shape\n",
    "    assert shape[0] == size or shape[1] == size, 'found image {}, control {}'.format(shape, ctrl)\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-258dc1dec356>, line 233)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-258dc1dec356>\"\u001b[0;36m, line \u001b[0;32m233\u001b[0m\n\u001b[0;31m    }\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ControlTextModel:\n",
    "    # If session is not defined than default session will be used\n",
    "    def __init__(self, a3c_model, session, encoder, repr_dim = 200):\n",
    "        self.word_repr = None\n",
    "        self.word_embeddings = None\n",
    "                \n",
    "        self.a3c_model = a3c_model\n",
    "        self.session = session \n",
    "        self.encoder = encoder\n",
    "        self.repr_dim = repr_dim\n",
    "        \n",
    "        self.device = '/cpu:0'\n",
    "        \n",
    "        self.add_text_inputs()\n",
    "        self.add_cnn_char_repr()\n",
    "        self.add_pretrained_word_embeddings()\n",
    "        self.add_context_repr(200)\n",
    "        self.add_loss()\n",
    "        self.add_training_op()\n",
    "        \n",
    "    \n",
    "    def add_img_repr(self):\n",
    "        with tf.device(self.device):\n",
    "            self.img_repr = slim.fully_connected(self.a3c_model.net, self.repr_dim)\n",
    "    \n",
    "    \n",
    "    def add_text_inputs(self):\n",
    "        with tf.variable_scope(\"char_repr\") as scope:\n",
    "            # shape = (batch size, sentence, word)\n",
    "            self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None], name=\"char_ids\")\n",
    "\n",
    "            # shape = (batch_size, sentence)\n",
    "            self.word_lengths = tf.placeholder(tf.int32, shape=[None, None], name=\"word_lengths\")\n",
    "\n",
    "        with tf.variable_scope(\"word_repr\") as scope:\n",
    "            # shape = (batch size)\n",
    "            self.sentence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sentence_lengths\")\n",
    "\n",
    "        with tf.variable_scope(\"training\", reuse=None) as scope:\n",
    "            # shape = (batch, 2)\n",
    "            # One hot labels if a3c_model.img is similar to current sentence\n",
    "            self.similar = tf.placeholder(tf.float32, shape=[None, 2], name=\"similar\")\n",
    "            \n",
    "\n",
    "            self.lr = tf.placeholder_with_default(0.005,  shape=(), name=\"lr\")\n",
    "            self.dropout = tf.placeholder_with_default(1., shape=(), name=\"dropout\")\n",
    "        \n",
    "                \n",
    "    def add_cnn_char_repr(self, nchars = 101, dim=25, nfilters=25, pad=2):\n",
    "        with tf.device(self.device):\n",
    "        \n",
    "            with tf.variable_scope(\"char_repr_cnn\") as scope:\n",
    "                # 1. Lookup for character embeddings\n",
    "                char_range = math.sqrt(3 / dim)\n",
    "                embeddings = tf.get_variable(name=\"char_embeddings\", dtype=tf.float32,\n",
    "                    shape=[nchars, dim],\n",
    "                    initializer=tf.random_uniform_initializer(-char_range, char_range))\n",
    "\n",
    "                # shape = (batch, sentence, word_len, embeddings dim)\n",
    "                char_embeddings = tf.nn.embedding_lookup(embeddings, self.char_ids)\n",
    "                char_embeddings = tf.nn.dropout(char_embeddings, self.dropout)\n",
    "                s = tf.shape(char_embeddings)\n",
    "\n",
    "                # shape = (batch x sentence, word_len, embeddings dim)\n",
    "                char_embeddings = tf.reshape(char_embeddings, shape=[-1, s[-2], dim])\n",
    "\n",
    "                # batch x sentence, word_len, nfilters\n",
    "                conv1d = tf.layers.conv1d(\n",
    "                    char_embeddings,\n",
    "                    filters=nfilters,\n",
    "                    kernel_size=[3],\n",
    "                    padding='same',\n",
    "                    activation=tf.nn.relu\n",
    "                )\n",
    "\n",
    "                # Max across each filter, shape = (batch x sentence, nfilters)\n",
    "                char_repr = tf.reduce_max(conv1d, axis=1, keep_dims=True)\n",
    "                char_repr = tf.squeeze(char_repr, squeeze_dims=[1])\n",
    "\n",
    "                # (batch, sentence, nfilters)\n",
    "                char_repr = tf.reshape(char_repr, shape=[s[0], s[1], nfilters])\n",
    "\n",
    "                if self.word_repr is not None:\n",
    "                    self.word_repr = tf.concat([self.word_repr, char_rep], axis=-1)\n",
    "                else:\n",
    "                    self.word_repr = char_repr\n",
    "\n",
    "    \n",
    "    \n",
    "    def add_pretrained_word_embeddings(self, dim=100, trainable=True):\n",
    "        with tf.device(self.device):\n",
    "            with tf.variable_scope(\"word_repr\") as scope:\n",
    "                # shape = (batch size, sentence, dim)\n",
    "                self.word_embeddings = tf.placeholder(tf.float32, shape=[None, None, dim],\n",
    "                                                      name=\"word_embeddings\")\n",
    "\n",
    "                if self.word_repr is not None:\n",
    "                    self.word_repr = tf.concat([self.word_repr, self.word_embeddings], axis=-1)\n",
    "                else:\n",
    "                    self.word_repr = word_embeddings\n",
    "    \n",
    "    \n",
    "    def extract_last(self, source, lengths):\n",
    "        batch_range = tf.range(tf.shape(source)[0])\n",
    "        batch_indices = tf.stack([batch_range, lengths - 1], axis=1)\n",
    "        res = tf.gather_nd(source, batch_indices)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    # Adds LSTM with size of each cell hidden_size\n",
    "    def add_context_repr(self, hidden_size=200):\n",
    "        with tf.device(self.device):\n",
    "        \n",
    "            with tf.variable_scope(\"context_repr\") as scope:\n",
    "                cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "                \n",
    "                word_repr = tf.nn.dropout(self.word_repr, self.dropout)\n",
    "\n",
    "                output, state = tf.nn.dynamic_rnn(\n",
    "                    cell,\n",
    "                    word_repr,\n",
    "                    sequence_length=self.sentence_lengths,\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "                context_repr = tf.nn.dropout(output, self.dropout)\n",
    "                \n",
    "                # batch x hidden_size\n",
    "                sentence_repr = self.extract_last(context_repr, self.sentence_lengths)\n",
    "                \n",
    "#                 w_bound = math.sqrt(6 / (hidden_size))\n",
    "#                 W = tf.get_variable(\"W\", shape=[hidden_size, self.repr_dim],\n",
    "#                                 dtype=tf.float32,\n",
    "#                                 initializer=tf.random_uniform_initializer(-w_bound, w_bound))\n",
    "\n",
    "\n",
    "#                 b = tf.get_variable(\"b\", shape=[self.repr_dim], dtype=tf.float32)\n",
    "                \n",
    "                # batch x hidden_size\n",
    "                self.text_repr = slim.fully_connected(sentence_repr, self.repr_dim)\n",
    "    \n",
    "    \n",
    "    def add_loss(self):\n",
    "        same = tf.losses.cosine_distance(self.text_repr, self.img_repr)\n",
    "        not_same = 1 - same\n",
    "        \n",
    "        probas = tf.stack([same, not_same])\n",
    "        \n",
    "        sim_loss = self.similar * tf.log(probas)\n",
    "        sim_loss = tf.reduce_sum(sim_loss, -1)\n",
    "        \n",
    "        self.text_loss = tf.reduce_mean(sim_loss)\n",
    "    \n",
    "    \n",
    "    # clip_gradient < 0  - no gradient clipping\n",
    "    def add_training_op(self, clip_gradient = 5.0):\n",
    "\n",
    "        with tf.variable_scope(\"training\", reuse=None) as scope:\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=self.lr, momentum=0.9)\n",
    "            if clip_gradient > 0:\n",
    "                gvs = optimizer.compute_gradients(self.loss)\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -clip_gradient, clip_gradient), var) for \n",
    "                              grad, var in gvs]\n",
    "                self.train_op = optimizer.apply_gradients(capped_gvs)\n",
    "            else:\n",
    "                self.train_op = optimizer.minimize(self.loss)\n",
    "\n",
    "            self.init_op = tf.variables_initializer(tf.global_variables(), name=\"init\")\n",
    "    \n",
    "    \n",
    "    def get_control_text(self, ctrl):\n",
    "        label = ctrl.get('label', '').strip()\n",
    "        tip = ctrl.get('tip', '').strip()\n",
    "        \n",
    "        result = label if label != '' else tip\n",
    "        return result.lower()\n",
    "    \n",
    "    \n",
    "    def is_for_training(self, ctrl):\n",
    "        txt = self.get_control_text(ctrl)\n",
    "        return txt != ''\n",
    "    \n",
    "    \n",
    "    def is_similar(self, ctrl1, ctrl2):\n",
    "        text1 = self.get_control_text(ctrl1)\n",
    "        text2 = self.get_control_text(ctrl2)\n",
    "        \n",
    "        tokens1 = set(word_tokenize(text1))\n",
    "        tokens2 = set(word_tokenize(text2))\n",
    "        intesection = len(tokens1.intesect(tokens2))\n",
    "        \n",
    "        scale = intesection / min(len(tokens1), len(tokens2))\n",
    "                    \n",
    "        return scale >= 1 / 2\n",
    "    \n",
    "    \n",
    "    def extract_pairs(self, controls, neg_samples = 5):\n",
    "        result = []\n",
    "        for ctrl in controls:\n",
    "            result.append((ctrl, ctrl, True))\n",
    "            \n",
    "            for i in range(neg_samples):\n",
    "                for _ in range(5):\n",
    "                    neg_ctrl = random.sample(controls)\n",
    "                    if not self.is_similar(ctrl, neg_ctrl):\n",
    "                        result.append(ctrl, neg_ctrl, False)\n",
    "                        break                    \n",
    "                    \n",
    "        \n",
    "        random.shuffle(result)\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def batch_to_input(self, batch):\n",
    "        imgs = []\n",
    "        texts = []\n",
    "        similarities = []\n",
    "        \n",
    "        for ctrl, ctrl2, similar in batch:\n",
    "            imgs.append(read_img(ctrl))\n",
    "            texts.append(self.get_control_text(ctrl2))\n",
    "            \n",
    "            sim = [1, 0] if similar else [0, 1]\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        text_input = self.encoder.encode(texts)\n",
    "        \n",
    "        return {\n",
    "            self.a3c_model.img: imgs,\n",
    "            self.similar: similarities,\n",
    "            \n",
    "            self.char_ids: text_input['char_ids'],\n",
    "            self.word_embeddings: text_input['word_embeddings'],\n",
    "            self.sentence_lengths: text_input['sentence_lengths'],\n",
    "            selt.word_lengths: text_input['word_lengths']\n",
    "        }\n",
    "        \n",
    "        \n",
    "    \n",
    "    def train(self, controls, word_embeddings, epochs = 5, neg_samples = 5):\n",
    "        to_train = list(filter(lambda ctrl: self.is_for_training(ctrl), controls))\n",
    "        \n",
    "        batch_size = 5\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            pairs = self.extract_pairs(controls, neg_samples)\n",
    "            \n",
    "            #for batch in split(pairs, 15):\n",
    "                \n",
    "                \n",
    "import gensim\n",
    "import numpy as np\n",
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    \n",
    "    def load_glove(self, glove_file):\n",
    "        with open(glove_file,'r') as f:\n",
    "            self.glove = {}\n",
    "            for line in f:\n",
    "                splitLine = line.split()\n",
    "                word = splitLine[0]\n",
    "                embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "                self.embeddings_dim = embedding.shape[0]\n",
    "                \n",
    "                self.glove[word] = embedding\n",
    "    \n",
    "    \n",
    "    def __init__(self, glove_file):\n",
    "        self.load_glove(glove_file)\n",
    "        self.char2id = {c: i + 1 for i, c in enumerate(string.printable)}\n",
    "        \n",
    "        self.empty = np.zeros(self.embeddings_dim)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def encode(self, sentences):\n",
    "        tokenized_sentence = list([word_tokenize(sentence.lower()) for sentence in sentences])\n",
    "        batch_size = len(sentences)\n",
    "        \n",
    "        max_sentence = 0\n",
    "        max_token = 0\n",
    "        for sentence in tokenized_sentence:\n",
    "            max_sentence = max(max_sentence, len(sentence))\n",
    "            for token in sentence:\n",
    "                max_token = max(max_token, len(token))\n",
    "        \n",
    "        word_embeddings = np.zeros((batch_size, max_sentence, self.embeddings_dim))\n",
    "        char_ids = np.zeros((batch_size, max_sentence, max_token))\n",
    "        sentence_lengths = np.zeros(batch_size)\n",
    "        word_lengths = np.zeros((batch_size, max_sentence))\n",
    "        \n",
    "        for i, sentence in enumerate(tokenized_sentence):\n",
    "            sentence_lengths[i] = len(sentence)\n",
    "            \n",
    "            for j, token in enumerate(sentence):\n",
    "                word_lengths[i, j] = len(token)\n",
    "                word_embeddings[i, j, :] = self.glove.get(token, self.empty)\n",
    "                \n",
    "                for k, char in enumerate(token):\n",
    "                    cid = self.char2id.get(char, 0)\n",
    "                    char_ids[i, j, k] = cid\n",
    "        \n",
    "        return {\n",
    "            'word_embeddings': word_embeddings,\n",
    "            'char_ids': char_ids,\n",
    "            'sentence_lengths': sentence_lengths,\n",
    "            'word_lengths': word_lengths\n",
    "        }\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder('glove.6B/glove.6B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': array([[[ 0.3038   ,  0.18126  ,  0.46583  , ...,  0.29551  ,\n",
       "          -0.25326  ,  0.72633  ],\n",
       "         [ 0.6223   , -0.1426   , -0.16478  , ...,  0.16229  ,\n",
       "          -0.33931  , -0.060914 ],\n",
       "         [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "           0.       ,  0.       ],\n",
       "         [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "           0.       ,  0.       ],\n",
       "         [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "           0.       ,  0.       ],\n",
       "         [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "           0.       ,  0.       ]],\n",
       " \n",
       "        [[ 0.15807  ,  0.24769  ,  0.20726  , ...,  0.52299  ,\n",
       "          -0.095942 , -0.0017835],\n",
       "         [ 0.73603  ,  0.52326  ,  0.65198  , ...,  0.40265  ,\n",
       "          -0.33904  ,  0.81648  ],\n",
       "         [-0.14931  ,  0.39057  , -0.44791  , ...,  0.074758 ,\n",
       "           0.23269  , -0.026688 ],\n",
       "         [ 0.0055743,  0.24747  ,  0.451    , ...,  0.030439 ,\n",
       "           0.47877  ,  0.12243  ],\n",
       "         [ 0.17651  ,  0.29208  , -0.0020768, ..., -0.20774  ,\n",
       "          -0.23189  , -0.10814  ],\n",
       "         [ 0.8685   ,  1.5325   ,  1.1066   , ...,  0.32541  ,\n",
       "          -0.44394  ,  0.29832  ]]]),\n",
       " 'chars': array([[[23., 35.,  0.,  0.,  0.,  0.],\n",
       "         [22., 19., 30., 30., 22., 15.],\n",
       "         [26., 25., 24., 24., 35.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.]],\n",
       " \n",
       "        [[15., 24., 30., 15., 28.,  0.],\n",
       "         [35., 25., 31., 28.,  0.,  0.],\n",
       "         [16., 19., 28., 29., 30.,  0.],\n",
       "         [24., 11., 23., 15.,  0.,  0.],\n",
       "         [74.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [26., 22., 15., 11., 29., 15.]]]),\n",
       " 'sentence_lengths': array([3., 6.]),\n",
       " 'word_lengths': array([[2., 6., 5., 0., 0., 0.],\n",
       "        [5., 4., 5., 4., 1., 6.]])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.encode(['My little ponny', 'Enter your first name, please'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlsModel:\n",
    "    def __init__(self, a3c_model):\n",
    "        self.a3c_model = a3c_model\n",
    "        self.session = a3c_model.session\n",
    "        self.opt = tf.train.AdamOptimizer(a3c_model.lr)\n",
    "        \n",
    "        \n",
    "    def add_types(self):\n",
    "        num_controls = len(Types.all_types)\n",
    "\n",
    "        self.control_types = tf.placeholder(tf.int32, (None), \"control_types\")\n",
    "                \n",
    "        he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\")\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        zero_init = tf.constant_initializer(0)\n",
    "        \n",
    "        l2_reg = slim.l2_regularizer(self.a3c_model.l2)\n",
    "        \n",
    "        with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                              weights_initializer = xavier_init,\n",
    "                              biases_initializer = zero_init,\n",
    "                              weights_regularizer = l2_reg\n",
    "                             ):\n",
    "\n",
    "            fc2 = slim.fully_connected(self.a3c_model.net, 100, weights_initializer=he_init)\n",
    "            flat = slim.dropout(fc2, self.a3c_model.dropout, scope='dropout')\n",
    "\n",
    "            self.types_logits = slim.fully_connected(flat, num_controls, activation_fn=None)\n",
    "            \n",
    "            self.types_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels = self.control_types, \n",
    "                logits = self.types_logits)\n",
    "            \n",
    "            self.types_loss = tf.reduce_mean(self.types_loss)\n",
    "            \n",
    "            self.types_train = self.opt.minimize(self.types_loss)\n",
    "            \n",
    "    \n",
    "    def split(self, arr, batch_size):\n",
    "        batch = []\n",
    "        for item in arr:\n",
    "            if len(batch) >= batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "            \n",
    "            batch.append(item)\n",
    "        \n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "            \n",
    "    \n",
    "    def batch_as_feed_dict(self, batch, lr, dropout, l2):\n",
    "        imgs = []\n",
    "        types = []\n",
    "        for ctrl in batch:\n",
    "            img = misc.imread(ctrl['img_file'])\n",
    "            img = (img - 128.)/128.\n",
    "            shape = img.shape\n",
    "            if shape[0] != 224 or shape[1] != 224:\n",
    "                print('skip, image: {}, control {}'.format(shape, ctrl))\n",
    "                continue\n",
    "\n",
    "            imgs.append(img)\n",
    "\n",
    "            ctrl_type = Types.all_types.index(ctrl['type'])\n",
    "            types.append(ctrl_type)\n",
    "\n",
    "        feed_dict = {\n",
    "            self.a3c_model.img: imgs,\n",
    "            self.a3c_model.lr: lr,\n",
    "            self.a3c_model.dropout: dropout,\n",
    "            self.a3c_model.l2: l2,\n",
    "\n",
    "            self.control_types: types                \n",
    "        }\n",
    "        \n",
    "        return feed_dict\n",
    "    \n",
    "    def train_types(self, controls, batch_size = 10, lr = 0.01, dropout = 0.7, l2 = 0.001, print_loss = True):\n",
    "        random.shuffle(controls)\n",
    "        \n",
    "        batches = self.split(controls, batch_size)\n",
    "        for batch in batches:\n",
    "            feed_dict = self.batch_as_feed_dict(batch, lr, dropout, l2)\n",
    "            \n",
    "            _, loss = self.session.run([self.types_train, self.types_loss], feed_dict = feed_dict)\n",
    "            \n",
    "            if print_loss:\n",
    "                print(loss)\n",
    "    \n",
    "    def measure_types_acc(self, controls):\n",
    "        batches = self.split(controls, 10)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in batches:\n",
    "            feed_dict = self.batch_as_feed_dict(batch, 0.1, 1., 0.1)\n",
    "            \n",
    "            logits = self.session.run(self.types_logits, feed_dict = feed_dict)\n",
    "            predicted = np.argmax(logits, axis = -1)\n",
    "            total += len(predicted)\n",
    "            correct += sum(predicted == feed_dict[self.control_types])\n",
    "        \n",
    "        if total == 0:\n",
    "            return 0.\n",
    "        \n",
    "        return float(correct) / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.Session()\n",
    "\n",
    "a3c = A3CModel(len(Actions.actions), session = session)\n",
    "a3c.init()\n",
    "\n",
    "model = ControlsModel(a3c)\n",
    "model.add_types()\n",
    "\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = []\n",
    "with open('dataset.jsonl') as f:\n",
    "    for line in f:\n",
    "        ctrl = json.loads(line)\n",
    "        controls.append(ctrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.measure_types_acc(controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    model.train_types(controls, batch_size = 10, lr = 0.01, dropout = 0.7)\n",
    "    model.measure_types_acc(controls)\n",
    "    print('acc: ', model.measure_types_acc(controls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = {}\n",
    "max_cnt = 0\n",
    "for ctrl in controls:\n",
    "    t = ctrl['type']\n",
    "    stat[t] = stat.get(t, 0) + 1\n",
    "    max_cnt = max(max_cnt, stat[t])\n",
    "\n",
    "print('baseline: ', max_cnt / len(controls))\n",
    "print('stat: ', stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.measure_types_acc(controls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
