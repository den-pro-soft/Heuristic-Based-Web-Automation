{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracing.rl.actions import *\n",
    "from tracing.rl.a3cmodel import A3CModel\n",
    "from tracing.rl.rewards import PopupRewardsCalculator\n",
    "from tracing.rl.environment import Environment\n",
    "from tracing.rl.actor_learner import ActionsMemory\n",
    "from tracing.rl.actor_learner import ActorLearnerWorker\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import threading\n",
    "import json\n",
    "import random\n",
    "from scipy import misc\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tracing.selenium_utils.controls import Types\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(arr, batch_size):\n",
    "    batch = []\n",
    "    for item in arr:\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "        batch.append(item)\n",
    "\n",
    "    if len(batch) > 0:\n",
    "        yield batch\n",
    "        \n",
    "        \n",
    "def read_img(ctrl, size = 300):\n",
    "    img = misc.imread(ctrl['img_file'])\n",
    "    img = (img - 128.)/128.\n",
    "    shape = img.shape\n",
    "    assert shape[0] == size or shape[1] == size, 'found image {}, control {}'.format(shape, ctrl)\n",
    "    \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import string \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    \n",
    "    def load_glove(self, glove_file):\n",
    "        with open(glove_file,'r') as f:\n",
    "            self.glove = {}\n",
    "            for line in f:\n",
    "                splitLine = line.split()\n",
    "                word = splitLine[0]\n",
    "                embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "                self.embeddings_dim = embedding.shape[0]\n",
    "                \n",
    "                self.glove[word] = embedding\n",
    "    \n",
    "    \n",
    "    def __init__(self, glove_file):\n",
    "        self.load_glove(glove_file)\n",
    "        self.char2id = {c: i + 1 for i, c in enumerate(string.printable)}\n",
    "        \n",
    "        self.empty = np.zeros(self.embeddings_dim)\n",
    "    \n",
    "    def encode(self, sentences):\n",
    "        tokenized_sentence = list([word_tokenize(sentence.lower()) for sentence in sentences])\n",
    "        batch_size = len(sentences)\n",
    "        \n",
    "        max_sentence = 0\n",
    "        max_token = 0\n",
    "        for sentence in tokenized_sentence:\n",
    "            max_sentence = max(max_sentence, len(sentence))\n",
    "            for token in sentence:\n",
    "                max_token = max(max_token, len(token))\n",
    "        \n",
    "        word_embeddings = np.zeros((batch_size, max_sentence, self.embeddings_dim))\n",
    "        char_ids = np.zeros((batch_size, max_sentence, max_token))\n",
    "        sentence_lengths = np.zeros(batch_size)\n",
    "        word_lengths = np.zeros((batch_size, max_sentence))\n",
    "        \n",
    "        for i, sentence in enumerate(tokenized_sentence):\n",
    "            sentence_lengths[i] = len(sentence)\n",
    "            \n",
    "            for j, token in enumerate(sentence):\n",
    "                word_lengths[i, j] = len(token)\n",
    "                word_embeddings[i, j, :] = self.glove.get(token, self.empty)\n",
    "                \n",
    "                for k, char in enumerate(token):\n",
    "                    cid = self.char2id.get(char, 0)\n",
    "                    char_ids[i, j, k] = cid\n",
    "        \n",
    "        return {\n",
    "            'word_embeddings': word_embeddings,\n",
    "            'char_ids': char_ids,\n",
    "            'sentence_lengths': sentence_lengths,\n",
    "            'word_lengths': word_lengths\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlTextModel:\n",
    "    # If session is not defined than default session will be used\n",
    "    def __init__(self, a3c_model, session, encoder, repr_dim = 200):\n",
    "        self.word_repr = None\n",
    "        self.word_embeddings = None\n",
    "                \n",
    "        self.a3c_model = a3c_model\n",
    "        self.session = session \n",
    "        self.encoder = encoder\n",
    "        self.repr_dim = repr_dim\n",
    "        \n",
    "        self.device = '/cpu:0'\n",
    "        \n",
    "        self.add_img_repr()\n",
    "        self.add_text_inputs()\n",
    "        self.add_cnn_char_repr()\n",
    "        self.add_pretrained_word_embeddings(encoder.embeddings_dim)\n",
    "        self.add_context_repr(200)\n",
    "        self.add_loss()\n",
    "        self.add_training_op()\n",
    "        \n",
    "    \n",
    "    def add_img_repr(self):\n",
    "        with tf.device(self.device):\n",
    "            self.img_repr = slim.fully_connected(self.a3c_model.net, self.repr_dim)\n",
    "    \n",
    "    \n",
    "    def add_text_inputs(self):\n",
    "        with tf.variable_scope(\"char_repr\") as scope:\n",
    "            # shape = (batch size, sentence, word)\n",
    "            self.char_ids = tf.placeholder(tf.int32, shape=[None, None, None], name=\"char_ids\")\n",
    "\n",
    "            # shape = (batch_size, sentence)\n",
    "            self.word_lengths = tf.placeholder(tf.int32, shape=[None, None], name=\"word_lengths\")\n",
    "\n",
    "        with tf.variable_scope(\"word_repr\") as scope:\n",
    "            # shape = (batch size)\n",
    "            self.sentence_lengths = tf.placeholder(tf.int32, shape=[None], name=\"sentence_lengths\")\n",
    "\n",
    "        with tf.variable_scope(\"training\", reuse=None) as scope:\n",
    "            # shape = (batch, 2)\n",
    "            # One hot labels if a3c_model.img is similar to current sentence\n",
    "            self.similar = tf.placeholder(tf.float32, shape=[None, 2], name=\"similar\")\n",
    "            \n",
    "\n",
    "            self.lr = tf.placeholder_with_default(0.005,  shape=(), name=\"lr\")\n",
    "            self.dropout = tf.placeholder_with_default(1., shape=(), name=\"dropout\")\n",
    "        \n",
    "                \n",
    "    def add_cnn_char_repr(self, nchars = 101, dim=25, nfilters=25, pad=2):\n",
    "        with tf.device(self.device):\n",
    "        \n",
    "            with tf.variable_scope(\"char_repr_cnn\") as scope:\n",
    "                # 1. Lookup for character embeddings\n",
    "                char_range = math.sqrt(3 / dim)\n",
    "                embeddings = tf.get_variable(name=\"char_embeddings\", dtype=tf.float32,\n",
    "                    shape=[nchars, dim],\n",
    "                    initializer=tf.random_uniform_initializer(-char_range, char_range))\n",
    "\n",
    "                # shape = (batch, sentence, word_len, embeddings dim)\n",
    "                char_embeddings = tf.nn.embedding_lookup(embeddings, self.char_ids)\n",
    "                char_embeddings = tf.nn.dropout(char_embeddings, self.dropout)\n",
    "                s = tf.shape(char_embeddings)\n",
    "\n",
    "                # shape = (batch x sentence, word_len, embeddings dim)\n",
    "                char_embeddings = tf.reshape(char_embeddings, shape=[-1, s[-2], dim])\n",
    "\n",
    "                # batch x sentence, word_len, nfilters\n",
    "                conv1d = tf.layers.conv1d(\n",
    "                    char_embeddings,\n",
    "                    filters=nfilters,\n",
    "                    kernel_size=[3],\n",
    "                    padding='same',\n",
    "                    activation=tf.nn.relu\n",
    "                )\n",
    "\n",
    "                # Max across each filter, shape = (batch x sentence, nfilters)\n",
    "                char_repr = tf.reduce_max(conv1d, axis=1, keep_dims=True)\n",
    "                char_repr = tf.squeeze(char_repr, squeeze_dims=[1])\n",
    "\n",
    "                # (batch, sentence, nfilters)\n",
    "                char_repr = tf.reshape(char_repr, shape=[s[0], s[1], nfilters])\n",
    "\n",
    "                if self.word_repr is not None:\n",
    "                    self.word_repr = tf.concat([self.word_repr, char_rep], axis=-1)\n",
    "                else:\n",
    "                    self.word_repr = char_repr\n",
    "\n",
    "    \n",
    "    \n",
    "    def add_pretrained_word_embeddings(self, dim=100, trainable=True):\n",
    "        with tf.device(self.device):\n",
    "            with tf.variable_scope(\"word_repr\") as scope:\n",
    "                # shape = (batch size, sentence, dim)\n",
    "                self.word_embeddings = tf.placeholder(tf.float32, shape=[None, None, dim],\n",
    "                                                      name=\"word_embeddings\")\n",
    "\n",
    "                if self.word_repr is not None:\n",
    "                    self.word_repr = tf.concat([self.word_repr, self.word_embeddings], axis=-1)\n",
    "                else:\n",
    "                    self.word_repr = word_embeddings\n",
    "    \n",
    "    \n",
    "    def extract_last(self, source, lengths):\n",
    "        batch_range = tf.range(tf.shape(source)[0])\n",
    "        batch_indices = tf.stack([batch_range, lengths - 1], axis=1)\n",
    "        res = tf.gather_nd(source, batch_indices)\n",
    "\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    # Adds LSTM with size of each cell hidden_size\n",
    "    def add_context_repr(self, hidden_size=200):\n",
    "        with tf.device(self.device):\n",
    "        \n",
    "            with tf.variable_scope(\"context_repr\") as scope:\n",
    "                cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "                \n",
    "                word_repr = tf.nn.dropout(self.word_repr, self.dropout)\n",
    "\n",
    "                output, state = tf.nn.dynamic_rnn(\n",
    "                    cell,\n",
    "                    word_repr,\n",
    "                    sequence_length=self.sentence_lengths,\n",
    "                    dtype=tf.float32)\n",
    "\n",
    "                context_repr = tf.nn.dropout(output, self.dropout)\n",
    "                \n",
    "                # batch x hidden_size\n",
    "                sentence_repr = self.extract_last(context_repr, self.sentence_lengths)\n",
    "                \n",
    "#                 w_bound = math.sqrt(6 / (hidden_size))\n",
    "#                 W = tf.get_variable(\"W\", shape=[hidden_size, self.repr_dim],\n",
    "#                                 dtype=tf.float32,\n",
    "#                                 initializer=tf.random_uniform_initializer(-w_bound, w_bound))\n",
    "\n",
    "\n",
    "#                 b = tf.get_variable(\"b\", shape=[self.repr_dim], dtype=tf.float32)\n",
    "                \n",
    "                # batch x hidden_size\n",
    "                self.text_repr = slim.fully_connected(sentence_repr, self.repr_dim)\n",
    "    \n",
    "    \n",
    "    def add_loss(self):\n",
    "        same = tf.losses.cosine_distance(self.text_repr, self.img_repr, axis=-1)\n",
    "        not_same = 1 - same\n",
    "        \n",
    "        probas = tf.stack([same, not_same])\n",
    "        \n",
    "        sim_loss = self.similar * tf.log(probas)\n",
    "        sim_loss = tf.reduce_sum(sim_loss, -1)\n",
    "        \n",
    "        self.text_loss = tf.reduce_mean(sim_loss)\n",
    "    \n",
    "    \n",
    "    # clip_gradient < 0  - no gradient clipping\n",
    "    def add_training_op(self, clip_gradient = 5.0):\n",
    "\n",
    "        with tf.variable_scope(\"training\", reuse=None) as scope:\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate=self.lr, momentum=0.9)\n",
    "            if clip_gradient > 0:\n",
    "                gvs = optimizer.compute_gradients(self.text_loss)\n",
    "                capped_gvs = [(tf.clip_by_value(grad, -clip_gradient, clip_gradient), var) for \n",
    "                              grad, var in gvs if grad is not None]\n",
    "                self.train_op = optimizer.apply_gradients(capped_gvs)\n",
    "            else:\n",
    "                self.train_op = optimizer.minimize(self.text_loss)\n",
    "\n",
    "            self.init_op = tf.variables_initializer(tf.global_variables(), name=\"init\")\n",
    "    \n",
    "    \n",
    "    def get_control_text(self, ctrl):\n",
    "        label = (ctrl.get('label') or '').strip()\n",
    "        tip = (ctrl.get('tip') or '').strip()\n",
    "        \n",
    "        result = label if label != '' else tip\n",
    "        return result.strip().lower()\n",
    "    \n",
    "    \n",
    "    def is_for_training(self, ctrl):\n",
    "        txt = self.get_control_text(ctrl)\n",
    "        return len(word_tokenize(txt)) > 0\n",
    "    \n",
    "    \n",
    "    def is_similar(self, ctrl1, ctrl2):\n",
    "        text1 = self.get_control_text(ctrl1)\n",
    "        text2 = self.get_control_text(ctrl2)\n",
    "        \n",
    "        tokens1 = set(word_tokenize(text1))\n",
    "        tokens2 = set(word_tokenize(text2))\n",
    "        intesection = len(tokens1.intersection(tokens2))\n",
    "        \n",
    "        scale = intesection / min(len(tokens1), len(tokens2))\n",
    "                    \n",
    "        return scale >= 1 / 2\n",
    "    \n",
    "    \n",
    "    def extract_pairs(self, controls, neg_samples = 5):\n",
    "        result = []\n",
    "                \n",
    "        for ctrl in controls:\n",
    "            result.append((ctrl, ctrl, True))\n",
    "            \n",
    "            for i in range(neg_samples):\n",
    "                for _ in range(neg_samples):\n",
    "                    neg_ctrl = random.choice(controls)\n",
    "                    if not self.is_similar(ctrl, neg_ctrl):\n",
    "                        result.append((ctrl, neg_ctrl, False))\n",
    "                        break                    \n",
    "                    \n",
    "        \n",
    "        random.shuffle(result)\n",
    "        return result\n",
    "        \n",
    "    \n",
    "    def batch_to_input(self, batch):\n",
    "        imgs = []\n",
    "        texts = []\n",
    "        similarities = []\n",
    "        \n",
    "        for ctrl, ctrl2, similar in batch:\n",
    "            imgs.append(read_img(ctrl))\n",
    "            texts.append(self.get_control_text(ctrl2))\n",
    "            \n",
    "            sim = [1, 0] if similar else [0, 1]\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        text_input = self.encoder.encode(texts)\n",
    "        \n",
    "        return {\n",
    "            self.a3c_model.img: imgs,\n",
    "            self.similar: similarities,\n",
    "            \n",
    "            self.char_ids: text_input['char_ids'],\n",
    "            self.word_embeddings: text_input['word_embeddings'],\n",
    "            self.sentence_lengths: text_input['sentence_lengths'],\n",
    "            self.word_lengths: text_input['word_lengths']\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def train(self, controls, epoch_start = 0, epoch_end = 5, neg_samples = 5, batch_size = 10):\n",
    "        to_train = list(filter(lambda ctrl: self.is_for_training(ctrl), controls))\n",
    "        \n",
    "        for epoch in range(epoch_start, epoch_end):\n",
    "            pairs = self.extract_pairs(to_train, neg_samples)\n",
    "            \n",
    "            print('epoch {} started'.format(epoch))\n",
    "            sum_loss = 0\n",
    "            for batch in split(pairs, batch_size):\n",
    "                feed = self.batch_to_input(batch)\n",
    "                _, loss = self.session.run([self.train_op, self.text_loss], feed_dict = feed)\n",
    "                sum_loss += loss / len(batch)\n",
    "            \n",
    "            print('epoch ended')\n",
    "            print('loss: {}'.format(sum_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder('glove.6B/glove.6B.200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder.encode(['My little ponny', 'Enter your first name, please'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3c = A3CModel(len(Actions.actions), session = session, train_deep=True)\n",
    "model = ControlTextModel(a3c, session, encoder, repr_dim = 200)\n",
    "\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = []\n",
    "with open('controls_dataset.jsonl') as f:\n",
    "    for line in f:\n",
    "        ctrl = json.loads(line)\n",
    "        controls.append(ctrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleksei/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "model.train(controls, epoch_start = 0, epoch_end = 5, neg_samples = 5, batch_size = 10)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
